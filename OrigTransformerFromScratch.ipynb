{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention is all you need - the original transformer from scratch\n",
        "\n",
        "The following notebook attempts to recreate the original transformer from the famous \"Attention is all you need\" paper, from scratch using PyTorch.\n",
        "\n",
        "The first part defines the model, through various classes for all the different subcomponents. The second part does some experimentation with the defined model. This part also uses a dataset from kaggle, which can be found in the following liondrive folder:\n",
        "\n",
        "https://drive.google.com/drive/folders/1DsMzK_HJE1vsNFXtlpTuhbdihWEDDPrQ?usp=sharing\n",
        "\n",
        "\n",
        "###Acknowledgments:\n",
        "\n",
        "- This notebook is partly adapted from the following medium article:\n",
        "https://medium.com/@sdwzh2725/transformer-code-step-by-step-understandingtransformer-d2ea773f15fa\n",
        "- The notebook also re-uses code snippets from various homeworks from the Applied Deep Learning course from Andrei Simion.\n",
        "- The dataset used in the second part comes from the following kaggle page:\n",
        "https://www.kaggle.com/datasets/bwandowando/479k-english-words?resource=download&select=words_alpha.txt"
      ],
      "metadata": {
        "id": "ddlAf7PHUqbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "-vHh9oEunXL8"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Define the model"
      ],
      "metadata": {
        "id": "2Mm1JwI6sGpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Embedding"
      ],
      "metadata": {
        "id": "bvEfFFb04vaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we define the embedding for each token. The embedding consists of two parts:\n",
        "- A vector of learnable weights, for each token in the vocabulary.\n",
        "- A (fixed) positional embedding providing information about the position of each token to the model.\n",
        "\n",
        "The embedding for each token is then defined as the sum of these two components."
      ],
      "metadata": {
        "id": "9BSd1DBm41vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "        Use the two formulas in the paper to calculate PositionalEmbedding\n",
        "\n",
        "        input size: [batch_size, seq_length]\n",
        "        return size: [batch_size, seq_length, d_model]\n",
        "\n",
        "        Args:\n",
        "            max_len: Maximum length of input sentence\n",
        "            dim_vector: the dimension of embedding vector for each input word.\n",
        "            vocab_size: amount of tokens in the vocabulary\n",
        "            drop_out: percentage of neurons to \"drop out\", i.e. set to zero. For regularization purposes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len, vocab_size, drop_out = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Learnable embedding: vector of learnable weights for each token\n",
        "        self.learnable_embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Positional embedding\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        positional_enc = torch.zeros(max_len, d_model)\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                positional_enc[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
        "                positional_enc[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i) / d_model)))\n",
        "\n",
        "        # The size of the pe matrix is [max_len, dim_vector].\n",
        "        print(f\"positional encoding sizeï¼š{positional_enc.size()}\")\n",
        "\n",
        "        # Register buffer, indicating that this parameter is not updated. Tips: Registering the buffer is equivalent\n",
        "        # to defining self.pe in__init__, so self.pe can be called in the forward function below, but the parameters\n",
        "        # will not be updated.\n",
        "        self.register_buffer('positional_enc', positional_enc)\n",
        "\n",
        "        # Dropout: for regularization\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The input x to the position code is [batch_size, seq_len], where seq_len is the length of the sentence\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        # Add learnable embedding and positional encoding for each token in the batch, and apply dropout.\n",
        "        return self.dropout(self.learnable_embed(x) + self.positional_enc[:seq_len, :])"
      ],
      "metadata": {
        "id": "HwUzA-Hq5hI4"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: cross-self-attention\n",
        "\n",
        "The encoder block employs cross-self-attention. First, we define one attention-head. Second, we combine multiple attention-heads to create a multi-head attention sublayer.\n",
        "\n",
        "First, a cross-attention-head:"
      ],
      "metadata": {
        "id": "Paa7N8KKFcct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents one head of cross-self-attention\n",
        "\n",
        "    input size: [batch_size, seq_length, d_model]\n",
        "    return size: [batch_size, seq_length, d_head]\n",
        "\n",
        "    Args:\n",
        "        d_head: dimension of each key, query and value vector.\n",
        "        d_model: embedding dimension\n",
        "        drop_out: percentage of attention weights to \"drop out\", i.e. set to zero. For regularization purposes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_head, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_head = d_head\n",
        "\n",
        "        # Matrices to map each embedding to a key, query and value of dimension d_head\n",
        "        self.W_K = nn.Linear(d_model, d_head, bias=False)\n",
        "        self.W_Q = nn.Linear(d_model, d_head, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_head, bias=False)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # B = batch_size, T = block_size, where block_size corresponds to the longest sequence length in the batch\n",
        "        B, T, d = x.shape\n",
        "\n",
        "        # Get the key and query representations from the embedding x\n",
        "        k = self.W_K(x)\n",
        "        q = self.W_Q(x)\n",
        "        v = self.W_V(x)\n",
        "\n",
        "        # Compute attention scores\n",
        "        # Transpose K is done over dimensions -2 and -1, i.e. (sequence_length, d_head)\n",
        "        scores = q @ k.transpose(-2,-1) / (self.d_head ** 0.5)\n",
        "\n",
        "        # Apply softmax to the final dimension of scores, i.e. over each row of each score matrix\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply dropout\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        # Calculate new representation for each token, using attention scores and value vectors\n",
        "        out = attention @ v\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ji04m31mGQhS"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, we define a multi-head-attention sublayer."
      ],
      "metadata": {
        "id": "QbGLm41-LxRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionEnc(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents a multi-head-attention sublayer, employing cross-attention heads.\n",
        "\n",
        "    input size: [batch_size, seq_length, d_model]\n",
        "    return size: [batch_size, seq_length, d_model]\n",
        "\n",
        "    Args:\n",
        "        d_head: dimension of each key, query and value vector in each attention head\n",
        "        d_model: embedding dimension\n",
        "        num_heads: number of attention heads\n",
        "        drop_out: drop_out in linear map after attention, for regularization.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_head, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Create num_heads attention heads\n",
        "        self.heads = nn.ModuleList([CrossAttentionHead(d_head, d_model) for _ in range(num_heads)])\n",
        "\n",
        "        # This is to project back to the dimension of d_model.\n",
        "        self.W_O = nn.Linear(d_head * num_heads, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Concatenate the different representations per head along the last dimension\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # Project the concatenation and apply dropout\n",
        "        out = self.dropout(self.W_O(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "Z4WZweHQLwkU"
      },
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Feed-Forward Neural Network\n",
        "\n",
        "Next, we define the feed-forward neural network that is used both in the encoder and decoder."
      ],
      "metadata": {
        "id": "4wHtaifWPR5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Feed forward neural network that takes in a representation for a certain position in the sequence,\n",
        "    and returns a new one. The same network is applied to each position in the sequence.\n",
        "\n",
        "    input size: [batch_size, seq_length, d_model]\n",
        "    return size: [batch_size, seq_length, d_model]\n",
        "\n",
        "    Args:\n",
        "        d_model: embedding dimension\n",
        "        drop_out: drop_out, for regularization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # As in \"Attention is all you need\" paper, hidden dimension is 4x embedding dimension.\n",
        "        d_hidden = 4 * d_model\n",
        "\n",
        "        # The simple feed forward net.\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.ff(x)"
      ],
      "metadata": {
        "id": "gLrXJgfIPhN_"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Define an encoder block\n",
        "\n",
        "Next, we can define an encoder block using the modules we defined above. We use the built in LayerNorm function from pytorch to implement the Add&Norm steps."
      ],
      "metadata": {
        "id": "73X3eyWeRVF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    This defines an Encoder Block. In the final encoder, multiple of these blocks are stacked on top of each other\n",
        "\n",
        "    input size: [batch_size, seq_length, d_model]\n",
        "    return size: [batch_size, seq_length, d_model]\n",
        "\n",
        "    Args:\n",
        "        d_model: embedding dimension\n",
        "        drop_out: drop_out, for regularization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super().__init__()\n",
        "        # We will assume that d_model is divisible by n_head (number of heads), to determine the dimension of each attention head.\n",
        "        d_head = d_model // n_head\n",
        "\n",
        "        # Define attention head, feed forward net, and layernorm layers.\n",
        "        self.multi_cross_att = MultiHeadAttentionEnc(n_head, d_head, d_model)\n",
        "        self.FFN = FeedForwardNet(d_model)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # add & norm after multi-head attention\n",
        "        x = self.ln1(x + self.multi_cross_att(x))\n",
        "        # add & norm after feed forward net\n",
        "        x = self.ln2(x + self.FFN(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "IpdIt8MbR1v5"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: define masked-self-attention.\n",
        "\n",
        "The decoder uses masked-self-attention. Here we thus define a masked-self-attention head."
      ],
      "metadata": {
        "id": "cmXtqWcTUBTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedSelfAttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents one head of cross-self-attention\n",
        "\n",
        "    input size: [batch_size, seq_length, d_model]\n",
        "    return size: [batch_size, seq_length, d_head]\n",
        "\n",
        "    Args:\n",
        "        d_head: dimension of each key, query and value vector.\n",
        "        d_model: embedding dimension\n",
        "        drop_out: percentage of attention weights to \"drop out\", i.e. set to zero. For regularization purposes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_head, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_head = d_head\n",
        "\n",
        "        # Matrices to map each embedding to a key, query and value of dimension d_head\n",
        "        self.W_K = nn.Linear(d_model, d_head, bias=False)\n",
        "        self.W_Q = nn.Linear(d_model, d_head, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_head, bias=False)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # B = batch_size, T = block_size, where block_size corresponds to the longest sequence length in the batch\n",
        "        B, T, d = x.shape\n",
        "\n",
        "        # Get the key and query representations from the embedding x\n",
        "        k = self.W_K(x)\n",
        "        q = self.W_Q(x)\n",
        "        v = self.W_V(x)\n",
        "\n",
        "        # Compute attention scores\n",
        "        # Transpose K is done over dimensions -2 and -1, i.e. (sequence_length, d_head)\n",
        "        scores = q @ k.transpose(-2,-1) / (self.d_head ** 0.5)\n",
        "\n",
        "        # Masked! Apply a mask to prevent attending future tokens\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1)  # Upper triangular mask\n",
        "        scores = scores.masked_fill(mask.bool(), float('-inf'))\n",
        "\n",
        "        # Apply softmax to the final dimension of scores, i.e. over each row of each score matrix\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply dropout\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        # Calculate new representation for each token, using attention scores and value vectors\n",
        "        out = attention @ v\n",
        "        return out"
      ],
      "metadata": {
        "id": "4ym5YoBFUhRW"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: masked multi-head attention\n",
        "\n",
        "Combine multiple masked self attention heads into one muli-head-attention head."
      ],
      "metadata": {
        "id": "OdtS8sBhV86K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedMultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents a masked multi-head-attention sublayer, employing masked self-attention heads.\n",
        "    This class is the same as the MuliHeadAttentionEnc class, only difference is use of MaskedSelfAttentionHeads\n",
        "    instead of CrossAttentionHead\n",
        "\n",
        "    input size: [batch_size, seq_length, d_model]\n",
        "    return size: [batch_size, seq_length, d_model]\n",
        "\n",
        "    Args:\n",
        "        d_head: dimension of each key, query and value vector in each attention head\n",
        "        d_model: embedding dimension\n",
        "        num_heads: number of attention heads\n",
        "        drop_out: drop_out in linear map after attention, for regularization.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_head, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Create num_heads attention heads\n",
        "        self.heads = nn.ModuleList([MaskedSelfAttentionHead(d_head, d_model) for _ in range(num_heads)])\n",
        "\n",
        "        # This is to project back to the dimension of d_model.\n",
        "        self.W_O = nn.Linear(d_head * num_heads, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Concatenate the different representations per head along the last dimension\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # Project the concatenation and apply dropout\n",
        "        out = self.dropout(self.W_O(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "TM1fewsya2F6"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Self-attention for decoder \\(second attention sublayer in decoder\\)\n",
        "\n",
        "Now we define the self-attention head for the second layer of the decoder. This head uses the encoder output to get the keys and values, and the output (after add&norm) from the previous attention sublayer in the encoder block."
      ],
      "metadata": {
        "id": "yfY3H-ORbYPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttentionHeadDec(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents one head of cross-attention in the decoder.\n",
        "\n",
        "    input size:\n",
        "      - x (queries from decoder): [batch_size, seq_length_dec, d_model]\n",
        "      - encoder_output (keys and values from encoder): [batch_size, seq_length_enc, d_model]\n",
        "\n",
        "    return size: [batch_size, seq_length_dec, d_head]\n",
        "\n",
        "    Args:\n",
        "        d_head: dimension of each key, query, and value vector.\n",
        "        d_model: embedding dimension.\n",
        "        dropout: percentage of attention weights to \"drop out\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_head, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_head = d_head\n",
        "\n",
        "        # Linear projections for queries, keys, and values\n",
        "        self.W_Q = nn.Linear(d_model, d_head, bias=False)  # Decoder input -> Queries\n",
        "        self.W_K = nn.Linear(d_model, d_head, bias=False)  # Encoder output -> Keys\n",
        "        self.W_V = nn.Linear(d_model, d_head, bias=False)  # Encoder output -> Values\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        B, T_dec, d = x.shape  # B = batch size, T_dec = decoder seq length, d = embedding dim\n",
        "        _, T_enc, _ = encoder_output.shape  # T_enc = encoder sequence length\n",
        "\n",
        "        # Compute queries (decoder), keys (encoder), and values (encoder)\n",
        "        q = self.W_Q(x)  # [B, T_dec, d_head]\n",
        "        k = self.W_K(encoder_output)  # [B, T_enc, d_head]\n",
        "        v = self.W_V(encoder_output)  # [B, T_enc, d_head]\n",
        "\n",
        "        # Compute attention scores (scaled dot-product attention)\n",
        "        scores = q @ k.transpose(-2, -1) / (self.d_head ** 0.5)\n",
        "\n",
        "        # Apply softmax to normalize the attention scores\n",
        "        attention = F.softmax(scores, dim=-1)  # [B, T_dec, T_enc]\n",
        "\n",
        "        # Apply dropout to attention scores\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        # Compute output as weighted sum of values\n",
        "        out = attention @ v\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "PWQ5-kGvcFGa"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Second multi-head-attention in decoder\n",
        "\n",
        "Next, we combine multiple heads into the second multi-head-attention sublayer in the decoder."
      ],
      "metadata": {
        "id": "RWAToXimdwo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionDec(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents the second multi-head-attention sublayer in the decoder.\n",
        "\n",
        "    input size:\n",
        "      - x (queries from decoder): [batch_size, seq_length_dec, d_model]\n",
        "      - encoder_output (keys and values from encoder): [batch_size, seq_length_enc, d_model]\n",
        "\n",
        "    return size: [batch_size, seq_length_dec, d_model]\n",
        "\n",
        "    Args:\n",
        "        d_head: dimension of each key, query, and value vector.\n",
        "        d_model: embedding dimension.\n",
        "        dropout: percentage of attention weights to \"drop out\".\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_head, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Create num_heads attention heads\n",
        "        self.heads = nn.ModuleList([CrossAttentionHeadDec(d_head, d_model) for _ in range(num_heads)])\n",
        "\n",
        "        # This is to project back to the dimension of d_model.\n",
        "        self.W_O = nn.Linear(d_head * num_heads, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        # Concatenate the different representations per head along the last dimension\n",
        "        out = torch.cat([h(x, encoder_output) for h in self.heads], dim=-1)\n",
        "\n",
        "        # Project the concatenation and apply dropout\n",
        "        out = self.dropout(self.W_O(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "25R18DxGeNzO"
      },
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: define a Decoder Block\n",
        "\n",
        "Here we define a decoder block. Multiple of these are stacked on top of each other in the decoder."
      ],
      "metadata": {
        "id": "RQ7d6xWMed_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    This defines a Decoder Block. In the final decoder, multiple of these blocks are stacked on top of each other\n",
        "\n",
        "    input size:\n",
        "      - x (queries from decoder): [batch_size, seq_length_dec, d_model]\n",
        "      - encoder_output (keys and values from encoder): [batch_size, seq_length_enc, d_model]\n",
        "\n",
        "    return size: [batch_size, seq_length_dec, d_model]\n",
        "\n",
        "    Args:\n",
        "        d_head: dimension of each key, query, and value vector.\n",
        "        d_model: embedding dimension.\n",
        "        dropout: percentage of attention weights to \"drop out\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super().__init__()\n",
        "        # We will assume that d_model is divisible by n_head (number of heads), to determine the dimension of each attention head.\n",
        "        d_head = d_model // n_head\n",
        "\n",
        "        # Define the two attention sublayers, feed forward net, and layernorm layers.\n",
        "        self.multi_masked_att = MaskedMultiHeadAttention(n_head, d_head, d_model)\n",
        "        self.multi_cross_att = MultiHeadAttentionDec(n_head, d_head, d_model)\n",
        "        self.FFN = FeedForwardNet(d_model)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ln3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_output):\n",
        "        # add & norm after masked multi-head attention\n",
        "        x = self.ln1(x + self.multi_masked_att(x))\n",
        "        # add & norm after second multi-head attention layer\n",
        "        x = self.ln2(x + self.multi_cross_att(x, enc_output))\n",
        "        # add & norm after feed forward net\n",
        "        x = self.ln3(x + self.FFN(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "LNoa2ySHen3e"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full transformer model\n",
        "\n",
        "Finally, we can now stack multiple encoder and decoder blocks to create the transformer model."
      ],
      "metadata": {
        "id": "riJOiRL7g3e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, max_len, vocab_size, n_layer, n_head,  drop_out = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = TokenEmbedding(d_model, max_len, vocab_size)\n",
        "        # Define encoder as multiple encoder blocks stacked on top of each other\n",
        "        self.encoder = nn.Sequential(*[EncoderBlock(d_model, n_head) for _ in range(n_layer)])\n",
        "        # Define decoder as multiple decoder blocks stacked on top of each other\n",
        "        self.decoder_blocks = nn.ModuleList([DecoderBlock(d_model, n_head) for _ in range(n_layer)])\n",
        "\n",
        "        # final linear layer to predict next token\n",
        "        self.lin = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input, output):\n",
        "        # get embeddings\n",
        "        x = self.embedding(input)\n",
        "        y = self.embedding(output)\n",
        "\n",
        "        # step through encoder\n",
        "        enc_output = self.encoder(x)\n",
        "        # step through decoder\n",
        "        dec_output = y\n",
        "        for decoder_block in self.decoder_blocks:\n",
        "            dec_output = decoder_block(dec_output, enc_output)\n",
        "\n",
        "\n",
        "        # apply final linear layer to get logits\n",
        "        logits = self.lin(dec_output)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "ZZod1thlhC77"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(d_model=256, max_len=6, vocab_size=10000, n_layer=3, n_head=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgZIe6rSlKIV",
        "outputId": "ea0c06ee-a4e7-446d-832e-afe049bec300"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positional encoding sizeï¼štorch.Size([6, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Experimenting with the model\n",
        "\n",
        "In this part we will do some small experimentations with the model. We will load a dataset of words, and train the transformer to reverse a word.\n",
        "\n",
        "I.e. given input \"hello\", the transformer should outpu \"elloh\""
      ],
      "metadata": {
        "id": "QNEuH_o2rxmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we import a dataset of english words. This dataset was found on kaggle:\n",
        "https://www.kaggle.com/datasets/bwandowando/479k-english-words?resource=download&select=words_alpha.txt\n",
        "\n",
        "Note, we will use the characters as our tokens for this application.\n",
        "\n",
        "From this dataset, we construct a mapping of words to integers and vice versa (stoi and itos)\n",
        "\n",
        "We pad the output with the special start and stop token \".\"\n",
        "\n",
        "For this code cell to work, you have to be running this notebook in google colab and have the dataset in your google drive. Also, this code cell will ask for access to your google drive."
      ],
      "metadata": {
        "id": "DMLu3dCcRmIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from collections import defaultdict\n",
        "\n",
        "# Dictionaries, {number -> character} and {character -> number}\n",
        "itos = defaultdict(int)\n",
        "stoi = defaultdict(int)\n",
        "\n",
        "# START = START token\n",
        "stoi['.'] = 0\n",
        "itos[0] = '.'\n",
        "encode = lambda s: [stoi[c] for c in s] # Encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([str(itos[i]) for i in l]) # Decoder: take a list of integers, output a string\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "f = open('/content/drive/MyDrive/words_alpha.txt', 'r')\n",
        "words = f.readlines()\n",
        "for word in words:\n",
        "    word = word.lower().strip()\n",
        "    for c in word:\n",
        "        if c not in stoi:\n",
        "            stoi[c] = len(stoi)\n",
        "            itos[len(itos)] = c\n",
        "            assert len(stoi) == len(itos)\n",
        "f.close()\n",
        "\n",
        "vocab_size = len(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4qNeegRruOY",
        "outputId": "df23687e-345e-492e-91ae-2e97cf6944ad"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a training dataset of pairs of words with their reversed version.\n",
        "For simplicity, we only take words of more than 1 and less than 5 characters."
      ],
      "metadata": {
        "id": "6lEEAIoQR5r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training dataset\n",
        "training_data = []\n",
        "for word in words:\n",
        "    word = word.lower().strip()\n",
        "\n",
        "    # only use words of more than 1 character\n",
        "    if len(word) > 1 and len(word) < 5:\n",
        "        # reverse word and add start and end token\n",
        "        reversed_word = '.' + word[::-1] + '.'\n",
        "        training_data.append((word, reversed_word))\n",
        "\n",
        "print(training_data[0:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKUf6N4rup6J",
        "outputId": "fb25b187-6a36-49bc-a8db-0aef3f7bdda6"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('aa', '.aa.'), ('aaa', '.aaa.'), ('aah', '.haa.'), ('aahs', '.shaa.'), ('aal', '.laa.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use TensorDataset and DataLoader to create batches of training examples."
      ],
      "metadata": {
        "id": "WXHXGnhNUEGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert training data to tensors\n",
        "input_seqs = [torch.tensor(encode(word)) for word, _ in training_data]\n",
        "output_seqs = [torch.tensor(encode(reversed_word)) for _, reversed_word in training_data]\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_len = max(len(seq) for seq in input_seqs + output_seqs)\n",
        "input_seqs = [torch.cat([seq, torch.zeros(max_len - len(seq), dtype=torch.int64)]) for seq in input_seqs]\n",
        "output_seqs = [torch.cat([seq, torch.zeros(max_len - len(seq), dtype=torch.int64)]) for seq in output_seqs]\n",
        "\n",
        "# Create TensorDataset and DataLoader\n",
        "dataset = TensorDataset(torch.stack(input_seqs), torch.stack(output_seqs))\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "x6oe4XFz43rs"
      },
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a Transformer model."
      ],
      "metadata": {
        "id": "KaW3LoBcSuTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(d_model=256, max_len=15, vocab_size=vocab_size, n_layer=3, n_head=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB-SGGQf3HvU",
        "outputId": "663602ec-48ca-4028-8e10-35a25d483fe2"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positional encoding sizeï¼štorch.Size([15, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train the model. To speed up this proces, we use cuda to load our model and training data to gpu, and run the training on the gpu."
      ],
      "metadata": {
        "id": "hyJKfOCgTGfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Create a PyTorch optimizer\n",
        "model = transformer\n",
        "model.to(device)\n",
        "learning_rate = 0.001\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "n_epochs = 1\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for batch_idx, (input_seq, output_seq) in enumerate(dataloader):\n",
        "        input_seq, output_seq = input_seq.to(device), output_seq.to(device)\n",
        "\n",
        "        logits = model(input_seq, output_seq)\n",
        "\n",
        "        # Calculate loss\n",
        "        logits = logits.view(-1, logits.shape[-1])\n",
        "        output_seq = output_seq.view(-1)\n",
        "        loss = F.cross_entropy(logits, output_seq, ignore_index=0)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
        "            scheduler.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWAhUhHg2qmA",
        "outputId": "e6c54c57-1b5a-47f2-e1de-0c3b7dfc8a93"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Batch 0, Loss: 3.429154396057129\n",
            "Epoch 0, Batch 50, Loss: 0.0031471741385757923\n",
            "Epoch 0, Batch 100, Loss: 0.0021119399461895227\n",
            "Epoch 0, Batch 150, Loss: 0.0015743699623271823\n",
            "Epoch 0, Batch 200, Loss: 0.001274883165024221\n",
            "Epoch 0, Batch 250, Loss: 0.0011024789419025183\n",
            "Epoch 0, Batch 300, Loss: 0.001051790197379887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4DtVKxvyUhpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the loss quickly drops and the model converges. However, we were not able to correctly generate an output from the given model. The issue might have been in the way that we attempted to generate from the model, but we are not entirely sure."
      ],
      "metadata": {
        "id": "8r82jGRhTe5t"
      }
    }
  ]
}