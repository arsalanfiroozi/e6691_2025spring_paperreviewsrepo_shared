# State-of-the-Art (SoTA) Methods in Attention and Efficiency

This repository provides links to papers, code repositories, and resources for cutting-edge methods discussed in our presentation. Below is a curated list of the latest advancements in attention mechanisms and model efficiency.

---

##  **Rethinking Attention with Performers**  
A novel approach to approximate self-attention with linear complexity, enabling faster training and inference for transformer models.  

- **Paper**: [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)  
- **Website**: [Google AI Blog Post](https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html)  
- **GitHub Repo**: [TensorFlow Implementation](https://github.com/google-research/google-research/tree/master/performer/fast_attention/tensorflow)  

---

##  **FlashAttention: Fast and Memory-Efficient Exact Attention**  
An IO-aware exact attention algorithm that optimizes memory usage and computational speed for transformers.  

- **Paper**: [FlashAttention](https://arxiv.org/abs/2205.14135)  
- **GitHub Repo**: [FlashAttention-1 Codebase](https://github.com/DL-Attention/flash-attention-1)  

---

##  **DeepSeek: Advancing Efficient and Scalable Models**  
A series of models focusing on improved training efficiency, architectural innovations, and scalability.  

### Papers:  
- **DeepSeek v2**: [Paper](https://arxiv.org/abs/2405.04434)  
- **DeepSeek v3**: [Paper](https://arxiv.org/abs/2412.19437)  

### Code:  
- **GitHub Repo**: [DeepSeek Official Repository](https://github.com/deepseek-ai)  

---

-DYH2111
