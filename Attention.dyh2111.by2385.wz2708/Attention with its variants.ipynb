{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f137bd",
   "metadata": {},
   "source": [
    "## Multi-head self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277f587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        \"\"\"\n",
    "        Initialize Self-Attention module.\n",
    "\n",
    "        Parameters:\n",
    "        - embed_size: Dimensionality of input embeddings.\n",
    "        - heads: Number of attention heads (multi-head attention splits embed_size into smaller heads).\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        # Number of heads for multi-head attention\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads  # Splitting embedding into smaller dimensions for each head\n",
    "        \n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by number of heads\"\n",
    "\n",
    "        # Learnable weight matrices for Query, Key, and Value transformations\n",
    "        self.W_q = nn.Linear(embed_size, embed_size, bias=False)  # Query weight matrix\n",
    "        self.W_k = nn.Linear(embed_size, embed_size, bias=False)  # Key weight matrix\n",
    "        self.W_v = nn.Linear(embed_size, embed_size, bias=False)  # Value weight matrix\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)  # Final output projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for Self-Attention.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, sequence_length, embed_size)\n",
    "\n",
    "        Returns:\n",
    "        - out: Self-attention output of shape (batch_size, sequence_length, embed_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, embed_size = x.shape\n",
    "\n",
    "        # Transform input embeddings into Q, K, V matrices\n",
    "        Q = self.W_q(x)  # Query matrix (batch_size, seq_length, embed_size)\n",
    "        K = self.W_k(x)  # Key matrix (batch_size, seq_length, embed_size)\n",
    "        V = self.W_v(x)  # Value matrix (batch_size, seq_length, embed_size)\n",
    "\n",
    "        # Reshape Q, K, V to (batch_size, heads, seq_length, head_dim) for multi-head processing\n",
    "        Q = Q.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute Scaled Dot-Product Attention\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))  # Dot product QK^T (batch_size, heads, seq_length, seq_length)\n",
    "        attention_scores = attention_scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))  # Scale by sqrt(d_k)\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # Apply softmax to normalize scores\n",
    "\n",
    "        # Compute attention-weighted sum of values\n",
    "        out = torch.matmul(attention_weights, V)  # (batch_size, heads, seq_length, head_dim)\n",
    "\n",
    "        # Reshape back to original shape (batch_size, seq_length, embed_size)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_size)\n",
    "\n",
    "        # Final linear transformation\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecdeb55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Output Shape: torch.Size([2, 5, 16])\n",
      "Self-Attention Output: tensor([[[ 4.0854e-01,  1.0211e-01, -1.9194e-01,  7.7359e-02,  4.2792e-01,\n",
      "          -4.5138e-01,  4.4197e-02, -3.5378e-01,  1.9782e-01, -1.0165e-01,\n",
      "          -2.4444e-01, -4.8325e-03,  1.3205e-01, -1.8331e-01, -3.3692e-01,\n",
      "          -1.3270e-01],\n",
      "         [ 4.0888e-01,  1.0327e-01, -1.9213e-01,  7.6321e-02,  4.2641e-01,\n",
      "          -4.5309e-01,  4.3546e-02, -3.5363e-01,  2.0017e-01, -1.0099e-01,\n",
      "          -2.4342e-01, -5.2806e-03,  1.3193e-01, -1.8099e-01, -3.3793e-01,\n",
      "          -1.3164e-01],\n",
      "         [ 4.0837e-01,  1.0174e-01, -1.9085e-01,  7.7388e-02,  4.2804e-01,\n",
      "          -4.5011e-01,  4.1277e-02, -3.5331e-01,  1.9723e-01, -1.0178e-01,\n",
      "          -2.4482e-01, -4.1396e-03,  1.3138e-01, -1.7882e-01, -3.3640e-01,\n",
      "          -1.3221e-01],\n",
      "         [ 4.0903e-01,  1.0549e-01, -1.9447e-01,  7.6325e-02,  4.2731e-01,\n",
      "          -4.5252e-01,  4.2496e-02, -3.5537e-01,  1.9656e-01, -1.0277e-01,\n",
      "          -2.4426e-01, -6.6773e-03,  1.3168e-01, -1.8140e-01, -3.3688e-01,\n",
      "          -1.3385e-01],\n",
      "         [ 4.0887e-01,  1.0309e-01, -1.9301e-01,  7.8263e-02,  4.2941e-01,\n",
      "          -4.5311e-01,  4.2868e-02, -3.5331e-01,  1.9667e-01, -1.0173e-01,\n",
      "          -2.4394e-01, -7.2789e-03,  1.3040e-01, -1.8309e-01, -3.3664e-01,\n",
      "          -1.3132e-01]],\n",
      "\n",
      "        [[ 4.7534e-01,  1.0717e-01, -1.6610e-01,  9.7050e-02,  4.6600e-01,\n",
      "          -4.8404e-01, -1.1889e-02, -3.8018e-01,  2.1416e-01, -1.5875e-01,\n",
      "          -2.5895e-01, -1.9262e-03,  5.4735e-02, -1.4704e-01, -3.7755e-01,\n",
      "          -1.1929e-01],\n",
      "         [ 4.7419e-01,  1.0552e-01, -1.6496e-01,  9.8073e-02,  4.6721e-01,\n",
      "          -4.8157e-01, -1.1710e-02, -3.7780e-01,  2.1272e-01, -1.5808e-01,\n",
      "          -2.5867e-01, -7.7288e-04,  5.4003e-02, -1.4831e-01, -3.7539e-01,\n",
      "          -1.1864e-01],\n",
      "         [ 4.7551e-01,  1.0584e-01, -1.6547e-01,  9.7827e-02,  4.6531e-01,\n",
      "          -4.8309e-01, -1.1860e-02, -3.7890e-01,  2.1502e-01, -1.5821e-01,\n",
      "          -2.5829e-01, -3.9768e-04,  5.5208e-02, -1.4650e-01, -3.7630e-01,\n",
      "          -1.1901e-01],\n",
      "         [ 4.7589e-01,  1.0670e-01, -1.6676e-01,  9.7321e-02,  4.6401e-01,\n",
      "          -4.8548e-01, -1.1826e-02, -3.7955e-01,  2.1703e-01, -1.5739e-01,\n",
      "          -2.5810e-01, -1.0904e-03,  5.6152e-02, -1.4640e-01, -3.7786e-01,\n",
      "          -1.1913e-01],\n",
      "         [ 4.7522e-01,  1.0783e-01, -1.6887e-01,  9.4797e-02,  4.5999e-01,\n",
      "          -4.9006e-01, -1.1292e-02, -3.8127e-01,  2.2202e-01, -1.5433e-01,\n",
      "          -2.5826e-01, -5.8825e-04,  6.0494e-02, -1.4772e-01, -3.8215e-01,\n",
      "          -1.2090e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    seq_length = 5\n",
    "    embed_size = 16\n",
    "    heads = 4\n",
    "\n",
    "    x = torch.rand(batch_size, seq_length, embed_size)  # Example input embeddings\n",
    "\n",
    "    self_attention = SelfAttention(embed_size, heads)\n",
    "    output = self_attention(x)\n",
    "\n",
    "    print(\"Self-Attention Output Shape:\", output.shape)  # Expected: (batch_size, seq_length, embed_size)    \n",
    "    print(\"Self-Attention Output:\", output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d39e1",
   "metadata": {},
   "source": [
    "## Masked self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6003cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        \"\"\"\n",
    "        Initialize Masked Self-Attention module.\n",
    "\n",
    "        Parameters:\n",
    "        - embed_size: Dimensionality of input embeddings.\n",
    "        - heads: Number of attention heads (multi-head attention splits embed_size into smaller heads).\n",
    "        \"\"\"\n",
    "        super(MaskedSelfAttention, self).__init__()\n",
    "        \n",
    "        # Number of heads for multi-head attention\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads  # Splitting embedding into smaller dimensions for each head\n",
    "        \n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by number of heads\"\n",
    "\n",
    "        # Learnable weight matrices for Query, Key, and Value transformations\n",
    "        self.W_q = nn.Linear(embed_size, embed_size, bias=False)  # Query weight matrix\n",
    "        self.W_k = nn.Linear(embed_size, embed_size, bias=False)  # Key weight matrix\n",
    "        self.W_v = nn.Linear(embed_size, embed_size, bias=False)  # Value weight matrix\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)  # Final output projection\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for Masked Self-Attention.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, sequence_length, embed_size)\n",
    "        - mask: Mask tensor of shape (batch_size, 1, seq_length, seq_length)\n",
    "\n",
    "        Returns:\n",
    "        - out: Masked self-attention output of shape (batch_size, sequence_length, embed_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, embed_size = x.shape\n",
    "\n",
    "        # Transform input embeddings into Q, K, V matrices\n",
    "        Q = self.W_q(x)  # Query matrix (batch_size, seq_length, embed_size)\n",
    "        K = self.W_k(x)  # Key matrix (batch_size, seq_length, embed_size)\n",
    "        V = self.W_v(x)  # Value matrix (batch_size, seq_length, embed_size)\n",
    "\n",
    "        # Reshape Q, K, V to (batch_size, heads, seq_length, head_dim) for multi-head processing\n",
    "        Q = Q.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute Scaled Dot-Product Attention\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))  # Dot product QK^T (batch_size, heads, seq_length, seq_length)\n",
    "        attention_scores = attention_scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))  # Scale by sqrt(d_k)\n",
    "\n",
    "        # Apply mask (set future words' scores to -inf)\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to normalize scores\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  \n",
    "\n",
    "        # Compute attention-weighted sum of values\n",
    "        out = torch.matmul(attention_weights, V)  # (batch_size, heads, seq_length, head_dim)\n",
    "\n",
    "        # Reshape back to original shape (batch_size, seq_length, embed_size)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_size)\n",
    "\n",
    "        # Final linear transformation\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf16999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a mask for causal self-attention\n",
    "def create_causal_mask(seq_length):\n",
    "    \"\"\"\n",
    "    Creates a lower triangular mask for causal (masked) self-attention.\n",
    "    \n",
    "    - Returns a tensor of shape (1, 1, seq_length, seq_length) where future tokens are masked.\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_length, seq_length))  # Lower triangular matrix (seq_length, seq_length)\n",
    "    return mask.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_length, seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a590c54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Self-Attention Output Shape: torch.Size([2, 5, 16])\n",
      "Masked Self-Attention Output: tensor([[[ 0.0433,  0.4703, -0.1431,  0.2472,  0.2905, -0.2199,  0.2942,\n",
      "           0.1567, -0.0184,  0.2952, -0.0145, -0.1265,  0.3437, -0.0480,\n",
      "           0.1510, -0.4225],\n",
      "         [ 0.0464,  0.3199, -0.0126,  0.1766,  0.3083, -0.1797,  0.3126,\n",
      "           0.2142,  0.0722,  0.3787, -0.1313, -0.0768,  0.3029,  0.0247,\n",
      "           0.1271, -0.5003],\n",
      "         [-0.0370,  0.2891, -0.0260,  0.1492,  0.2494, -0.1769,  0.2930,\n",
      "           0.2498,  0.0917,  0.3124, -0.1894, -0.1639,  0.2779,  0.0321,\n",
      "           0.1695, -0.5547],\n",
      "         [-0.0650,  0.2304, -0.0118,  0.1525,  0.2306, -0.1136,  0.3311,\n",
      "           0.2630,  0.0810,  0.3415, -0.1704, -0.2008,  0.2072,  0.0012,\n",
      "           0.1693, -0.4700],\n",
      "         [-0.0916,  0.2251, -0.0165,  0.1363,  0.2353, -0.1044,  0.3241,\n",
      "           0.2303,  0.0852,  0.2971, -0.1761, -0.2521,  0.1796, -0.0121,\n",
      "           0.1802, -0.4444]],\n",
      "\n",
      "        [[-0.0939,  0.2634, -0.0999,  0.1595,  0.0042, -0.2722,  0.2460,\n",
      "           0.2660,  0.0644,  0.2964, -0.2653, -0.3361,  0.1940,  0.0335,\n",
      "           0.2333, -0.4769],\n",
      "         [-0.0186,  0.3059, -0.0395,  0.0470,  0.1365, -0.2293,  0.2941,\n",
      "           0.2228,  0.0828,  0.3227, -0.3106, -0.2601,  0.1926,  0.0418,\n",
      "           0.1819, -0.4875],\n",
      "         [ 0.0183,  0.2761, -0.0174,  0.0983,  0.2195, -0.1754,  0.3476,\n",
      "           0.2038,  0.0088,  0.4226, -0.2086, -0.1888,  0.2005,  0.0221,\n",
      "           0.1507, -0.4073],\n",
      "         [-0.0324,  0.2648, -0.0360,  0.1092,  0.1752, -0.1531,  0.3476,\n",
      "           0.2514, -0.0029,  0.4166, -0.1945, -0.2160,  0.1884, -0.0019,\n",
      "           0.1633, -0.4262],\n",
      "         [-0.0578,  0.2712, -0.0373,  0.1232,  0.1832, -0.1336,  0.3286,\n",
      "           0.2457,  0.0114,  0.3758, -0.1693, -0.2173,  0.1988, -0.0178,\n",
      "           0.1638, -0.4328]]], grad_fn=<ViewBackward0>)\n",
      "Masked Self-Attention Mask: tensor([[[[1., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    seq_length = 5\n",
    "    embed_size = 16\n",
    "    heads = 4\n",
    "\n",
    "    x = torch.rand(batch_size, seq_length, embed_size)  # Example input embeddings\n",
    "    mask = create_causal_mask(seq_length)  # Generate causal mask\n",
    "\n",
    "    masked_self_attention = MaskedSelfAttention(embed_size, heads)\n",
    "    output = masked_self_attention(x, mask)\n",
    "\n",
    "    print(\"Masked Self-Attention Output Shape:\", output.shape)  # Expected: (batch_size, seq_length, embed_size)\n",
    "    print(\"Masked Self-Attention Output:\", output)  # Expected: (batch_size, seq_length, embed_size)\n",
    "    print(\"Masked Self-Attention Mask:\", mask)  # Expected: (batch_size, seq_length, embed_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8bcdc0",
   "metadata": {},
   "source": [
    "## Cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54cfe240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        \"\"\"\n",
    "        Initialize Cross-Attention module.\n",
    "\n",
    "        Parameters:\n",
    "        - embed_size: Dimensionality of input embeddings.\n",
    "        - heads: Number of attention heads (multi-head attention splits embed_size into smaller heads).\n",
    "        \"\"\"\n",
    "        super(CrossAttention, self).__init__()\n",
    "        \n",
    "        # Number of heads for multi-head attention\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads  # Splitting embedding into smaller dimensions for each head\n",
    "        \n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by number of heads\"\n",
    "\n",
    "        # Learnable weight matrices for Query (decoder), Key (encoder), and Value (encoder)\n",
    "        self.W_q = nn.Linear(embed_size, embed_size, bias=False)  # Query weight matrix (from decoder)\n",
    "        self.W_k = nn.Linear(embed_size, embed_size, bias=False)  # Key weight matrix (from encoder)\n",
    "        self.W_v = nn.Linear(embed_size, embed_size, bias=False)  # Value weight matrix (from encoder)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)  # Final output projection\n",
    "\n",
    "    def forward(self, decoder_x, encoder_x):\n",
    "        \"\"\"\n",
    "        Forward pass for Cross-Attention.\n",
    "\n",
    "        Parameters:\n",
    "        - decoder_x: Decoder input tensor of shape (batch_size, target_seq_length, embed_size)\n",
    "        - encoder_x: Encoder output tensor of shape (batch_size, source_seq_length, embed_size)\n",
    "\n",
    "        Returns:\n",
    "        - out: Cross-attention output of shape (batch_size, target_seq_length, embed_size)\n",
    "        \"\"\"\n",
    "        batch_size, target_seq_length, embed_size = decoder_x.shape\n",
    "        source_seq_length = encoder_x.shape[1]  # Length of the encoder's output sequence\n",
    "\n",
    "        # Transform decoder input into Query (Q), and encoder output into Key (K) and Value (V)\n",
    "        Q = self.W_q(decoder_x)  # Query (from decoder)\n",
    "        K = self.W_k(encoder_x)  # Key (from encoder)\n",
    "        V = self.W_v(encoder_x)  # Value (from encoder)\n",
    "\n",
    "        # Reshape Q, K, V for multi-head attention (batch_size, heads, seq_length, head_dim)\n",
    "        Q = Q.view(batch_size, target_seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, source_seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, source_seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute Scaled Dot-Product Attention\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))  # Dot product QK^T (batch_size, heads, target_seq_length, source_seq_length)\n",
    "        attention_scores = attention_scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))  # Scale by sqrt(d_k)\n",
    "\n",
    "        # Apply softmax to normalize scores\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  \n",
    "\n",
    "        # Compute attention-weighted sum of values\n",
    "        out = torch.matmul(attention_weights, V)  # (batch_size, heads, target_seq_length, head_dim)\n",
    "\n",
    "        # Reshape back to original shape (batch_size, target_seq_length, embed_size)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, target_seq_length, embed_size)\n",
    "\n",
    "        # Final linear transformation\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df3d434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Attention Output Shape: torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    source_seq_length = 6  # Encoder output length\n",
    "    target_seq_length = 4  # Decoder input length\n",
    "    embed_size = 16\n",
    "    heads = 4\n",
    "\n",
    "    encoder_output = torch.rand(batch_size, source_seq_length, embed_size)  # Example encoder output\n",
    "    decoder_input = torch.rand(batch_size, target_seq_length, embed_size)  # Example decoder input\n",
    "\n",
    "    cross_attention = CrossAttention(embed_size, heads)\n",
    "    output = cross_attention(decoder_input, encoder_output)\n",
    "\n",
    "    print(\"Cross-Attention Output Shape:\", output.shape)  # Expected: (batch_size, target_seq_length, embed_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627c908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
